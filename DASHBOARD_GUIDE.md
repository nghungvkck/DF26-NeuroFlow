# ğŸ“Š Dashboard User Guide

## Overview

Interactive Streamlit dashboard vá»›i **3 visualization modes** Ä‘á»ƒ phÃ¢n tÃ­ch káº¿t quáº£ autoscaling pipeline.

---

## ğŸš€ Quick Start

### 1. Cháº¡y Pipeline (táº¡o dá»¯ liá»‡u)
```bash
python run_pipeline.py
```

### 2. Khá»Ÿi Ä‘á»™ng Dashboard
```bash
streamlit run dashboard/app.py
```

### 3. Truy cáº­p
Má»Ÿ browser táº¡i: **http://localhost:8501**

---

## ğŸ¯ Visualization Modes

### **Mode 1: Autoscaling Tests** (Phase B)

Test cÃ¡c chiáº¿n lÆ°á»£c autoscaling trÃªn synthetic scenarios.

#### **7 Tabs:**

1. **ğŸ“Š Load & Forecast**
   - Traffic pattern vs predicted load
   - Forecast accuracy visualization
   - Error distribution

2. **ğŸ“ˆ Pod Timeline**
   - Pod scaling decisions over time
   - Strategy comparison
   - Scaling events count

3. **ğŸ’° Cost Analysis**
   - Cumulative cost by strategy
   - Cost efficiency comparison
   - Resource utilization

4. **ğŸš¨ SLA Violations**
   - Service breach timeline
   - Violation statistics per strategy
   - Impact quantification

5. **ğŸ“‹ Metrics Comparison**
   - Comprehensive metrics table
   - Total cost, avg pods, violations
   - Scaling events, utilization stats

6. **ğŸ”´ Anomaly Detection** (NEW!)
   - Anomaly timeline with markers
   - Detection statistics by strategy
   - Anomaly types distribution
   - Scaling response to anomalies

7. **ğŸ¯ Advanced Metrics** (NEW!)
   - Kubernetes HPA metrics (CPU utilization, target breaches)
   - AWS Auto Scaling metrics (warm-up, cooldown effectiveness)
   - Cost model comparison (simple, cloud, K8s, Borg)

#### **Filters:**
- **Scenario**: Select load pattern (Gradual Increase, Sudden Spike, etc.)
- **Strategies**: Choose strategies to compare (multi-select)

---

### **Mode 2: Model Evaluation** (Phase A)

ÄÃ¡nh giÃ¡ forecast models trÃªn real historical data.

#### **Content:**
- **Best Models**: Top performing model per timeframe (1m, 5m, 15m)
- **Detailed Metrics**: MAE, RMSE, MAPE by model
- **Comparison**: LSTM vs XGBoost vs Hybrid

---

### **Mode 3: Anomaly & Cost Analysis** (Phase C - NEW!)

Advanced analysis vá» anomaly detection vÃ  cost optimization.

#### **3 Tabs:**

1. **ğŸ”´ Anomaly Detection**
   - Performance metrics by anomaly type:
     - DDoS Attack
     - Flash Sale
     - Service Failure
     - Thundering Herd
     - Multi-region Failover
   - F1 Score, Precision, Recall comparison
   - Detection rate visualization
   - Key insights vÃ  recommendations

2. **ğŸ’° Cost Models**
   - Cost comparison across 5 models:
     - Simple Linear (baseline)
     - Cloud Mixed (AWS/GCP/Azure style)
     - Kubernetes (node packing)
     - Borg Production
     - Borg Batch
   - Cost breakdown by component (reserved, on-demand, spot, startup)
   - Savings visualization
   - Kubernetes packing efficiency metrics

3. **ğŸ“Š Platform Metrics**
   - **Kubernetes HPA**: CPU utilization, target breaches, trigger rate
   - **AWS Auto Scaling**: Warm-up ratio, cooldown effectiveness
   - Platform best practices vÃ  insights

---

## ğŸ“‚ Required Files

Dashboard Ä‘á»c dá»¯ liá»‡u tá»« `results/` directory:

### **Phase A (Model Evaluation):**
```
results/model_evaluation.json
```

### **Phase B (Autoscaling Tests):**
```
results/simulation_results.csv
results/metrics_summary.json
```

### **Phase C (Anomaly & Cost Analysis):**
```
results/anomaly_analysis.json
results/cost_breakdown.json
```

### **Summary:**
```
results/pipeline_summary.json
```

---

## ğŸ¨ Dashboard Features

### **Interactive Charts:**
- âœ… Hover tooltips for detailed info
- âœ… Zoom and pan capabilities
- âœ… Legend toggle (click to hide/show)
- âœ… Export to PNG

### **Data Tables:**
- âœ… Sortable columns
- âœ… Full-width responsive design
- âœ… Formatted numbers (currency, percentages)

### **Filters:**
- âœ… Scenario selector
- âœ… Multi-strategy comparison
- âœ… Real-time updates

---

## ğŸ”§ Troubleshooting

### **"âš ï¸ No results found"**
```bash
# Run pipeline first
python run_pipeline.py
```

### **"âš ï¸ Phase C results not found"**
```bash
# Run Phase C analysis
python run_pipeline.py --phase-c-only
```

### **"Advanced metrics not available"**
Cháº¡y simulation vá»›i `enable_advanced_metrics=True` (default trong pipeline má»›i)

### **Dashboard khÃ´ng load:**
```bash
# Check terminal for errors
# Reinstall streamlit if needed:
pip install --upgrade streamlit plotly pandas
```

---

## ğŸ’¡ Tips & Best Practices

### **Performance:**
- Load dá»¯ liá»‡u tá»« 1 scenario trÆ°á»›c khi compare táº¥t cáº£
- Use filter Ä‘á»ƒ giáº£m data points hiá»ƒn thá»‹
- Close unused tabs trong browser

### **Analysis:**
- **Compare strategies** trÃªn same scenario Ä‘á»ƒ fair comparison
- **Check Anomaly Detection** tab Ä‘á»ƒ hiá»ƒu scaling behavior trong extreme events
- **Review Cost Models** Ä‘á»ƒ tÃ¬m cost optimization opportunities

### **Interpretation:**
- **F1 > 0.8**: Excellent anomaly detection
- **Packing Efficiency > 80%**: Good Kubernetes node utilization
- **Cooldown Effectiveness > 70%**: AWS Auto Scaling working well
- **Savings > 30%**: Significant cost optimization potential

---

## ğŸ“Š Example Analysis Workflow

1. **Start with Autoscaling Tests**
   - Select "GRADUAL_INCREASE" scenario
   - Compare all 4 strategies
   - Check Metrics Comparison tab

2. **Deep Dive into Anomaly Detection**
   - Switch to Tab 6 - Anomaly Detection
   - Identify anomaly patterns
   - Check scaling response rates

3. **Analyze Cost Optimization**
   - Switch to Mode 3 - Anomaly & Cost Analysis
   - Review Cost Models tab
   - Compare savings across models

4. **Platform-Specific Tuning**
   - Check Advanced Metrics (Tab 7)
   - Review K8s HPA metrics
   - Optimize AWS cooldown settings

---

## ğŸ¯ Key Metrics to Watch

### **Autoscaling Performance:**
- Total Cost: Lower is better
- SLA Violations: Should be 0 or minimal
- Scaling Events: Fewer = more stable
- Mean Utilization: 70-85% is optimal

### **Anomaly Detection:**
- F1 Score: > 0.7 is good, > 0.8 is excellent
- Precision: Minimize false positives
- Recall: Catch all real anomalies

### **Cost Optimization:**
- Savings vs Simple: Target > 20%
- Packing Efficiency: Target > 75%
- Wasted Capacity: Target < 25%

---

## ğŸš€ Next Steps

1. **Experiment with different scenarios** trong Autoscaling Tests
2. **Compare cost models** trong Anomaly & Cost Analysis
3. **Tune strategies** based on metrics
4. **Export charts** for presentations
5. **Share insights** vá»›i team

---

**Dashboard Version:** 3.0 (vá»›i Anomaly Detection & Cost Analysis)
**Last Updated:** January 31, 2026
