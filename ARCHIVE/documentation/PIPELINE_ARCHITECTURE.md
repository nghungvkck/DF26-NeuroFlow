# Pipeline Analysis & Architecture

## ğŸ“Š Tá»•ng Quan Pipeline

Há»‡ thá»‘ng Ä‘Æ°á»£c thiáº¿t káº¿ vá»›i **3 pha** xá»­ lÃ½:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          AUTOSCALING OPTIMIZATION PIPELINE                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  PHASE A: MODEL EVALUATION (Dá»¯ liá»‡u thá»±c)                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚
â”‚  Input: data/real/*.csv (lá»‹ch sá»­ traffic)                â”‚
â”‚  Processing:                                              â”‚
â”‚    â”œâ”€ Load real data                                      â”‚
â”‚    â”œâ”€ Evaluate LSTM, XGBoost, Hybrid models              â”‚
â”‚    â””â”€ Compute MAE, RMSE, MAPE metrics                    â”‚
â”‚  Output: results/model_evaluation.json                    â”‚
â”‚                                                             â”‚
â”‚  â–¼                                                           â”‚
â”‚                                                             â”‚
â”‚  PHASE B: AUTOSCALING SCENARIO TESTING (Dá»¯ liá»‡u synthetic) â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  Input: Generated scenarios (5 types)                      â”‚
â”‚  Processing:                                              â”‚
â”‚    â”œâ”€ Generate 5 ká»‹ch báº£n táº£i (gradual, spike, etc)     â”‚
â”‚    â”œâ”€ Test 4 strategies (reactive, predictive, etc)      â”‚
â”‚    â”œâ”€ For each combo: Run simulate.py                    â”‚
â”‚    â”‚  â”œâ”€ Dá»± bÃ¡o táº£i (forecaster)                        â”‚
â”‚    â”‚  â”œâ”€ Quyáº¿t Ä‘á»‹nh scaling (autoscaler)                â”‚
â”‚    â”‚  â”œâ”€ TÃ­nh metrics (cost, SLA, stability)            â”‚
â”‚    â”‚  â””â”€ PhÃ¡t hiá»‡n anomaly                               â”‚
â”‚    â””â”€ Aggregate káº¿t quáº£                                  â”‚
â”‚  Output: results/simulation_results.csv                   â”‚
â”‚          results/metrics_summary.json                     â”‚
â”‚                                                             â”‚
â”‚  â–¼                                                           â”‚
â”‚                                                             â”‚
â”‚  PHASE C: ADVANCED ANALYSIS (Anomaly & Cost)              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”‚
â”‚  Input: Simulation results                                â”‚
â”‚  Processing:                                              â”‚
â”‚    â”œâ”€ Test anomaly detection (DDoS, flash sales)         â”‚
â”‚    â”œâ”€ Evaluate cost models (K8s, AWS, GCP)              â”‚
â”‚    â””â”€ Measure platform metrics                           â”‚
â”‚  Output: results/anomaly_analysis.json                    â”‚
â”‚          results/cost_breakdown.json                      â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Chi Tiáº¿t Quy TrÃ¬nh Trong Má»—i Pha

### PHASE A: Model Evaluation (forecast/model_evaluation.py)

**Má»¥c Ä‘Ã­ch:** ÄÃ¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c cá»§a cÃ¡c ML models trÃªn dá»¯ liá»‡u thá»±c.

```python
# Quy trÃ¬nh:
1. Load dá»¯ liá»‡u thá»±c tá»« data/real/*.csv
   â””â”€ train_1m.csv, train_5m.csv, train_15m.csv (dá»¯ liá»‡u huáº¥n luyá»‡n)
   â””â”€ test_1m.csv, test_5m.csv, test_15m.csv (dá»¯ liá»‡u kiá»ƒm tra)

2. Cho má»—i timeframe (1m, 5m, 15m):
   â”œâ”€ Load model Ä‘Ã£ Ä‘Æ°á»£c lÆ°u tá»« models/
   â”‚  â””â”€ LSTM: lstm_1m.keras
   â”‚  â””â”€ XGBoost: xgboost_1m_model.json
   â”‚  â””â”€ Hybrid: (LSTM + residual learning)
   â”‚
   â”œâ”€ Dá»± bÃ¡o trÃªn test set
   â”‚  â””â”€ TÃ­nh MAE, RMSE, MAPE
   â”‚
   â””â”€ XÃ¡c Ä‘á»‹nh mÃ´ hÃ¬nh tá»‘t nháº¥t

3. Output: model_evaluation.json
   {
     "1m": {"best_model": "LSTM", "mae": 5.2, "rmse": 7.1},
     "5m": {"best_model": "XGBoost", "mae": 12.3, "rmse": 15.6},
     ...
   }
```

---

### PHASE B: Autoscaling Testing (simulate.py)

**Má»¥c Ä‘Ã­ch:** Test cÃ¡c autoscaling strategies trÃªn ká»‹ch báº£n táº¿.

#### B1: Táº¡o Scenarios (autoscaling/scenarios.py)

```python
# 5 ká»‹ch báº£n test:
1. GRADUAL_INCREASE
   â””â”€ Load tÄƒng dáº§n: tá»« 50 â†’ 300 requests/s
   â””â”€ Kiá»ƒm tra policy tÄƒng pod tá»« tá»«

2. SUDDEN_SPIKE
   â””â”€ Load tÄƒng Ä‘á»™t ngá»™t: 100 â†’ 400 requests/s
   â””â”€ Kiá»ƒm tra kháº£ nÄƒng pháº£n á»©ng nhanh

3. OSCILLATING
   â””â”€ Load dao Ä‘á»™ng: 100 â†” 300 â†” 100 requests/s
   â””â”€ Kiá»ƒm tra trÃ¡nh "flapping" (scaling liÃªn tá»¥c)

4. TRAFFIC_DROP
   â””â”€ Load giáº£m Ä‘á»™t ngá»™t: 300 â†’ 50 requests/s
   â””â”€ Kiá»ƒm tra scale-down hiá»‡u quáº£

5. FORECAST_ERROR_TEST
   â””â”€ Load khÃ´ng theo pattern dá»± bÃ¡o
   â””â”€ Kiá»ƒm tra kháº£ nÄƒng chá»‹u lá»—i forecast
```

#### B2: Simulate Core Loop (simulate.py - run_strategy_on_scenario)

```python
for t in range(len(load_series)):  # Cho má»—i timestep
    
    # 1. Láº¤Y Dá»® LIá»†U THá»°C
    actual_requests = load_series[t]
    
    # 2. PHÃT HIá»†N ANOMALY
    is_anomaly = anomaly_detector.detect(actual_requests)
    
    # 3. Dá»° BÃO (Forecasting)
    forecast = forecaster.predict(history, horizon=1)
    
    # 4. QUYáº¾T Äá»ŠNH SCALING (Autoscaler)
    new_pods, action, reason = autoscaler.step(
        current_pods=current_pods,
        requests=actual_requests,
        forecast=forecast
    )
    
    # 5. TÃNH METRICS
    metrics.record(
        t=t,
        pods=new_pods,
        requests=actual_requests,
        scaling_action=action,
        sla_before_scaling=sla_violated_before
    )
    
    # 6. Cáº¬P NHáº¬T TRáº NG THÃI
    current_pods = new_pods
    records.append({
        'timestamp': t,
        'pods': new_pods,
        'requests': actual_requests,
        'forecast': forecast,
        'action': action,
        ...
    })

# 7. Tá»”NG Há»¢P Káº¾T QUáº¢
return {
    'strategy': strategy_name,
    'scenario': scenario_name,
    'records': records,
    'metrics': metrics.aggregate(),
    'total_cost': sum(pods_history),
    'sla_violations': violations_count,
    'scaling_events': actions_count
}
```

#### B3: 4 Autoscaling Strategies

```python
STRATEGY 1: REACTIVE (autoscaling/reactive.py)
â””â”€ Pháº£n á»©ng vá»›i táº£i hiá»‡n táº¡i
â””â”€ if requests > threshold: scale_up()
â””â”€ ÄÆ¡n giáº£n, Ä‘á»™ trá»… cao

STRATEGY 2: PREDICTIVE (autoscaling/predictive.py)
â””â”€ Dá»± bÃ¡o táº£i trong tÆ°Æ¡ng lai
â””â”€ if forecast_requests > threshold: scale_up()
â””â”€ Proactive, cáº§n dá»± bÃ¡o chÃ­nh xÃ¡c

STRATEGY 3: CPU_BASED (autoscaling/cpu_based.py)
â””â”€ Dá»±a trÃªn CPU utilization
â””â”€ if cpu_utilization > 70%: scale_up()
â””â”€ Truyá»n thá»‘ng, cÃ³ thá»ƒ over-provision

STRATEGY 4: HYBRID (autoscaling/hybrid.py)
â””â”€ Káº¿t há»£p nhiá»u yáº¿u tá»‘: requests, forecast, CPU, history
â””â”€ ThÃ­ch á»©ng, cÃ³ anti-flapping mechanism
â””â”€ Tá»‘i Æ°u nháº¥t cho production
```

#### B4: TÃ­nh Metrics (cost/metrics.py)

```python
Cho má»—i strategy Ã— scenario:

1. COST METRICS
   â”œâ”€ total_cost = sum(pod_count) Ã— cost_per_hour Ã— time_interval
   â””â”€ avg_pods = mean(pod_history)

2. SLA METRICS
   â”œâ”€ sla_violations = count(requests > capacity)
   â”œâ”€ sla_violation_rate = violations / total_steps
   â””â”€ time_to_handle = thá»i gian Ä‘á»ƒ xá»­ lÃ½ SLA

3. STABILITY METRICS
   â”œâ”€ scaling_events = count(pods changed)
   â”œâ”€ oscillation_count = count(scale up then down)
   â””â”€ pod_change_rate = |pods_t - pods_t-1|

4. K8S HPA METRICS
   â”œâ”€ cpu_utilization = requests / capacity
   â”œâ”€ target_tracking_events = count(target breached)
   â””â”€ warm_up_time = thá»i gian instance warming

5. AWS AUTO SCALING METRICS
   â”œâ”€ cooldown_effectiveness = pods released / pods_max
   â””â”€ warm_up_overhead = extra capacity during warm-up
```

---

### PHASE C: Advanced Analysis (anomaly + cost)

```python
1. ANOMALY DETECTION TESTING
   â”œâ”€ Inject anomalies: DDoS, flash sales, failovers
   â”œâ”€ Test detection: Z-score, IQR, rate-change
   â””â”€ Measure detection accuracy

2. COST MODEL TESTING
   â”œâ”€ Kubernetes cost model
   â”œâ”€ AWS EC2 cost model
   â”œâ”€ Google Cloud cost model
   â””â”€ Spot instances (dynamic pricing)

3. OUTPUT
   â””â”€ results/anomaly_analysis.json
   â””â”€ results/cost_breakdown.json
```

---

## âš™ï¸ Objective Function (Multi-Objective Optimization)

```python
# autoscaling/objective.py

MINIMIZE: w_cost Ã— Cost + w_sla Ã— SLA + w_stability Ã— Stability

Chi tiáº¿t tá»«ng component:

1. COST COMPONENT
   Cost = Î£(pods_t Ã— cost_per_hour Ã— step_hours)
   â””â”€ Má»¥c tiÃªu: Sá»­ dá»¥ng Ã­t pods nháº¥t

2. SLA COMPONENT
   SLA_t = 1 náº¿u requests_t > pods_t Ã— capacity
   SLA_cost = Î£(SLA_t) Ã— penalty
   â””â”€ Má»¥c tiÃªu: TrÃ¡nh SLA violations

3. STABILITY COMPONENT
   Scaling_events = Î£(|action_t|)
   Stability_cost = Î£(scaling_events) Ã— penalty
   â””â”€ Má»¥c tiÃªu: TrÃ¡nh flapping (scaling liÃªn tá»¥c)

4. AGGREGATION (máº·c Ä‘á»‹nh weights = {cost: 1, sla: 1, stability: 1})
   Total = 1.0 Ã— Cost + 1.0 Ã— SLA + 1.0 Ã— Stability
```

---

## ğŸš¨ CÃC Váº¤NÄá»€ ÄÆ¯á»¢C PHÃT HIá»†N & TÃŒNH TRáº NG

### Váº¥n Äá» 1: SLA Violation Logic (ÄÃ£ fix âœ…)
**Problem:** SLA Ä‘Æ°á»£c tÃ­nh SAU khi scaling, nÃªn luÃ´n = 0
**NguyÃªn nhÃ¢n:** TÃ­nh toÃ¡n SLA sau khi pods Ä‘Ã£ tÄƒng
**Fix:** ThÃªm metric `sla_before_scaling` Ä‘á»ƒ track SLA TRÆ¯á»šC decision
**TÃ¬nh tráº¡ng:** âœ… ÄÃ£ fix trong MetricsCollector

### Váº¥n Äá» 2: Real Data vs Synthetic Data Mixing (ÄÃ£ fix âœ…)
**Problem:** Autoscaling tests cháº¡y trÃªn dá»¯ liá»‡u thá»±c, gÃ¢y nháº§m láº«n
**NguyÃªn nhÃ¢n:** KhÃ´ng tÃ¡ch rÃµ PHASE A vÃ  PHASE B
**Fix:** Táº¡o model_evaluation.py (PHASE A) riÃªng, simulate.py chá»‰ cháº¡y synthetic
**TÃ¬nh tráº¡ng:** âœ… ÄÃ£ fix trong run_pipeline.py

### Váº¥n Äá» 3: Forecaster Integration (ÄÃ£ fix âœ…)
**Problem:** Sá»­ dá»¥ng ARIMA khÃ´ng tá»‘i Æ°u
**NguyÃªn nhÃ¢n:** ARIMA cháº­y vÃ  khÃ´ng chÃ­nh xÃ¡c cho short-term forecast
**Fix:** Thay báº±ng ML models (LSTM, XGBoost, Hybrid)
**TÃ¬nh tráº¡ng:** âœ… ÄÃ£ fix trong model_forecaster.py

### Váº¥n Äá» 4: Capacity Per Pod (Cáº§n review âš ï¸)
**Current value:** capacity_per_pod = 100 requests/s
**Problem:** GiÃ¡ trá»‹ nÃ y cÃ³ há»£p lÃ½ khÃ´ng? Test data cháº¡y ~300 requests max
**Impact:** Náº¿u quÃ¡ tháº¥p â†’ quÃ¡ nhiá»u SLA, quÃ¡ cao â†’ khÃ´ng tháº¥y SLA
**Recommendation:** Xem dá»¯ liá»‡u thá»±c Ä‘á»ƒ calibrate

---

## ğŸ“ˆ Data Flow (Chi Tiáº¿t)

```
DATA SOURCES
â”œâ”€â”€ data/real/
â”‚   â”œâ”€â”€ train_1m.csv â”€â”€â”
â”‚   â”œâ”€â”€ train_5m.csv  â”€â”¼â”€â†’ PHASE A: ModelEvaluator
â”‚   â”œâ”€â”€ train_15m.csv â”€â”¤   â”‚
â”‚   â”œâ”€â”€ test_1m.csv   â”€â”¤   â””â”€â†’ model_evaluation.json
â”‚   â”œâ”€â”€ test_5m.csv   â”€â”¤
â”‚   â””â”€â”€ test_15m.csv â”€â”€â”˜

â””â”€â”€ autoscaling/scenarios.py
    â””â”€â†’ generate_all_scenarios()
        â””â”€â†’ PHASE B: simulate.py
            â”œâ”€ run_strategy_on_scenario()
            â”œâ”€ run_strategy_on_scenario()
            â””â”€ run_strategy_on_scenario() Ã— (4 strategies Ã— 5 scenarios)
            
FORECASTING
â”œâ”€â”€ models/lstm_*.keras
â”œâ”€â”€ models/xgboost_*.json
â””â”€â”€ forecast/model_forecaster.py
    â””â”€â†’ Predict load for next step

AUTOSCALING DECISIONS
â”œâ”€â”€ autoscaling/reactive.py      â†’ Decision making
â”œâ”€â”€ autoscaling/predictive.py    â”‚
â”œâ”€â”€ autoscaling/cpu_based.py     â”‚
â””â”€â”€ autoscaling/hybrid.py        â†“

METRICS COLLECTION
â””â”€â”€ cost/metrics.py
    â”œâ”€â”€ Cost: Î£(pods Ã— cost_per_hour)
    â”œâ”€â”€ SLA: violations count
    â”œâ”€â”€ Stability: scaling events
    â””â”€â”€ Platform metrics (K8s, AWS)
    
OUTPUT
â”œâ”€â”€ results/simulation_results.csv      (Detailed records)
â”œâ”€â”€ results/metrics_summary.json        (Aggregated metrics)
â”œâ”€â”€ results/strategy_comparison.json    (Winner analysis)
â”œâ”€â”€ results/model_evaluation.json       (PHASE A)
â””â”€â”€ results/anomaly_analysis.json       (PHASE C)
```

---

## ğŸ”§ CÃ¡ch Cháº¡y Pipeline

```bash
# RUN ALL PHASES
python run_pipeline.py

# RUN SPECIFIC PHASE
python run_pipeline.py --phase-a-only    # Model evaluation only
python run_pipeline.py --phase-b-only    # Autoscaling tests only
python run_pipeline.py --phase-c-only    # Anomaly & cost analysis

# RUN SIMULATE DIRECTLY (PHASE B chá»‰)
python simulate.py

# VIEW DASHBOARD
streamlit run dashboard/app.py
```

---

## ğŸ“Š Output Files

```
results/
â”œâ”€â”€ model_evaluation.json
â”‚   â””â”€ Accuracy metrics (MAE, RMSE, MAPE) per model per timeframe
â”‚
â”œâ”€â”€ simulation_results.csv
â”‚   â””â”€ Row = 1 timestep Ã— 1 scenario Ã— 1 strategy
â”‚   â””â”€ Columns: scenario, strategy, t, pods, requests, forecast, cost, sla, action
â”‚
â”œâ”€â”€ metrics_summary.json
â”‚   â””â”€ Aggregated metrics per strategy:
â”‚   â”‚  {
â”‚   â”‚    "REACTIVE": {
â”‚   â”‚      "total_cost": 1.74,
â”‚   â”‚      "avg_pods": 2.1,
â”‚   â”‚      "sla_violations": 0,
â”‚   â”‚      "scaling_events": 19
â”‚   â”‚    }
â”‚   â”‚  }
â”‚
â”œâ”€â”€ strategy_comparison.json
â”‚   â””â”€ Win count & ranking per strategy
â”‚
â”œâ”€â”€ anomaly_analysis.json
â”‚   â””â”€ Anomaly detection results (PHASE C)
â”‚
â””â”€â”€ cost_breakdown.json
    â””â”€ Cost by platform (K8s, AWS, GCP)
```

---

## âœ… Architecture Assessment

### Äiá»ƒm Tá»‘t âœ…

1. **Clear separation**: PHASE A, B, C tÃ¡ch rÃµ
2. **Reproducible**: Synthetic scenarios, fixed random seed
3. **Comprehensive metrics**: Cost, SLA, stability, platform-specific
4. **Multi-strategy comparison**: Fair comparison giá»¯a policies
5. **Real data integration**: Phase A Ä‘Ã¡nh giÃ¡ trÃªn dá»¯ liá»‡u thá»±c
6. **Production-ready**: Anti-flapping, multi-objective, hybrid policy

### Äiá»ƒm Cáº§n Cáº£i Thiá»‡n âš ï¸

1. **Capacity Per Pod**: Cáº§n calibrate dá»±a trÃªn real data
2. **Cost Parameters**: Needs tuning based on actual cloud costs
3. **Weights in Objective**: Currently equal, cÃ³ thá»ƒ cáº§n custom weights
4. **Forecaster Error Handling**: Fallback to heuristic náº¿u model fail
5. **Anomaly Threshold**: Z-score = 3.0 cÃ³ quÃ¡ cao?

---

**Káº¿t Luáº­n:** Pipeline ráº¥t tá»‘t, logic há»£p lÃ½. Chá»‰ cáº§n tuning parameters.
