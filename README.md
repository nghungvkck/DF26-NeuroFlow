# Dá»± Ã¡n: Autoscaling Forecasting & Optimization â€” Chá»§ Ä‘á»: Autoscaling

## 1. TÃ³m táº¯t

### Váº¥n Ä‘á» cáº§n giáº£i quyáº¿t

- **Chi phÃ­ háº¡ táº§ng Ä‘Ã¡m mÃ¢y cao**: Cáº¥p phÃ¡t tÄ©nh theo peak load lÃ£ng phÃ­ 60-70% tÃ i nguyÃªn
- **KhÃ³ dá»± Ä‘oÃ¡n táº£i**: Traffic biáº¿n Ä‘á»™ng theo giá», ngÃ y, sá»± kiá»‡n Ä‘áº·c biá»‡t
- **CÃ¢n báº±ng giá»¯a chi phÃ­ & hiá»‡u nÄƒm**: Tá»‘i thiá»ƒu hÃ³a chi phÃ­ mÃ  váº«n Ä‘áº£m báº£o SLA 99%+

### Ã tÆ°á»Ÿng vÃ  cÃ¡ch tiáº¿p cáº­n

- **Dá»± Ä‘oÃ¡n táº£i tÆ°Æ¡ng lai** báº±ng Machine Learning (XGBoost, LightGBM, Hybrid LSTM)
- **Tá»‘i Æ°u hÃ³a tÃ i nguyÃªn** qua hybrid autoscaling 4 lá»›p:
  - **Layer 0**: Anomaly Detection (phÃ¡t hiá»‡n báº¥t thÆ°á»ng)
  - **Layer 1**: Emergency Response (pháº£n á»©ng kháº©n cáº¥p khi CPU > 95%)
  - **Layer 2**: Predictive Scaling (dá»± tÃ­nh trÆ°á»›c tá»« forecast)
  - **Layer 3**: Reactive Scaling (theo thá»±c táº¿ hiá»‡n táº¡i)
- **Tá»‘i Æ°u chi phÃ­** vá»›i mÃ´ hÃ¬nh 3 loáº¡i instance: Reserved + Spot + On-Demand

### GiÃ¡ trá»‹ thá»±c tiá»…n

- **Giáº£m 25-35% chi phÃ­** so vá»›i cáº¥p phÃ¡t cá»‘ Ä‘á»‹nh
- **Äáº£m báº£o 99%+ SLA** - cam káº¿t cháº¥t lÆ°á»£ng dá»‹ch vá»¥
- **Tá»± Ä‘á»™ng thÃ­ch á»©ng** vá»›i táº£i Ä‘á»™t biáº¿n
- **á»¨ng dá»¥ng thá»±c táº¿** cho há»‡ thá»‘ng web, IoT, streaming

---

## 2. Dá»¯ liá»‡u

### Nguá»“n

- **Apache HTTP Server Logs** (~2.9 triá»‡u requests)
  - Thá»i gian: ThÃ¡ng 8/1995 (NASA Kennedy Space Center)
  - Äá»‹nh dáº¡ng: Apache Common Log Format
  - Train: 1-22/8 | Test: 23-31/8

### MÃ´ táº£ trÆ°á»ng dá»¯ liá»‡u chÃ­nh

| TrÆ°á»ng         | MÃ´ táº£                               | VÃ­ dá»¥               |
| -------------- | ----------------------------------- | ------------------- |
| timestamp      | Thá»i Ä‘iá»ƒm request                   | 1995-08-15 10:23:45 |
| host           | Äá»‹a chá»‰ IP khÃ¡ch                    | 192.168.1.1         |
| method         | HTTP method                         | GET, POST, HEAD     |
| url            | ÄÆ°á»ng dáº«n tÃ i nguyÃªn                | /index.html         |
| status         | MÃ£ HTTP                             | 200, 404, 500       |
| bytes          | Dung lÆ°á»£ng response                 | 1024                |
| requests_count | **LÆ°á»£ng request trong time window** | 500, 1200, 2000     |

### Tiá»n xá»­ lÃ½ Ä‘Ã£ thá»±c hiá»‡n

#### 1. **Missing Data Handling**

- TÄƒng bá»• sung missing timestamps (khoáº£ng trá»‘ng Aug 1-3)
- Interpolation: Linear, Forward Fill, Backward Fill tÃ¹y theo khoáº£ng

#### 2. **Outlier Detection & Removal**

- IQR (Interquartile Range) method: TÃ¬m outliers vÆ°á»£t quÃ¡ 1.5Ã—IQR
- ÄÃ¡nh dáº¥u cÃ¡c sá»± kiá»‡n báº¥t thÆ°á»ng (burst)

#### 3. **Normalization & Scaling**

- Min-Max scaling: $X' = \frac{X - X_{min}}{X_{max} - X_{min}}$
- Chuáº©n hÃ³a vá» [0, 1] Ä‘á»ƒ trÃ¡nh dominance cá»§a features lá»›n

#### 4. **Feature Engineering** (13 features)

| Loáº¡i         | TÃªn Feature                     | Má»¥c Ä‘Ã­ch                    |
| ------------ | ------------------------------- | --------------------------- |
| **Temporal** | hour_of_day                     | Chu ká»³ Ä‘Æ¡n vá»‹ (24h)         |
|              | day_of_week                     | Máº«u hÃ ng tuáº§n               |
|              | hour_sin, hour_cos              | Encode cyclic pattern       |
| **Lag**      | lag_requests_5m, 15m, 6h, 1d    | Phá»¥ thuá»™c thá»i gian quÃ¡ khá»© |
| **Burst**    | is_event, is_burst, burst_ratio | PhÃ¡t hiá»‡n tÄƒng táº£i Ä‘á»™t biáº¿n |
| **Rolling**  | rolling_mean_1h, rolling_max_1h | Xu hÆ°á»›ng 1 giá» gáº§n Ä‘Ã¢y      |

---

## 3. MÃ´ hÃ¬nh & Kiáº¿n trÃºc

### Kiáº¿n trÃºc tá»•ng thá»ƒ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Dá»¯ liá»‡u Ä‘áº§u vÃ o (Raw logs)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    Data Preprocessing Pipeline    â”‚
         â”‚  (Parser â†’ Normalizer â†’ Agg)      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                            â–¼
   3 Time Windows             Feature Engineering
   (1m, 5m, 15m)              (Temporal, Lag, Rolling)
        â”‚                            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Train/Test Split (80/20)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â–¼              â–¼              â–¼
  XGBoost       LightGBM         Hybrid
  (Gradient)    (Leaf-wise)   (LSTM+Prophet)
      â”‚              â”‚              â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Evaluation & Metrics        â”‚
        â”‚  (MAE, RMSE, MAPE, SMAPE)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â–¼              â–¼              â–¼
 Predictions    Metrics         Models
 (CSV)          (JSON/CSV)       (Serialized)
```

### MÃ´ hÃ¬nh sá»­ dá»¥ng

#### **1. XGBoost** (Gradient Boosting)

- **Æ¯u Ä‘iá»ƒm**: Nhanh, xá»­ lÃ½ features phi tuyáº¿n tá»‘t
- **Hyperparameters**:
  - learning_rate: 0.05
  - max_depth: 6
  - num_rounds: 1000

#### **2. LightGBM** (Light Gradient Boosting)

- **Æ¯u Ä‘iá»ƒm**: Tiáº¿t kiá»‡m bá»™ nhá»›, train nhanh hÆ¡n XGBoost
- **Hyperparameters**:
  - num_leaves: 63
  - max_depth: 6
  - bagging_fraction: 0.8

#### **3. Hybrid** (LSTM + Prophet)

- **LSTM**: Capture temporal dependencies, 2 layers, 64 hidden units
- **Prophet**: Seasonality detection (yearly, weekly, daily)
- **Ensemble**: Weighted average hoáº·c stacking

### Chiáº¿n lÆ°á»£c validation & training

```
Train Data (Aug 1-22)  â†’  Train Models (Cross-validation)
                            â†“
                    Grid Search / Random Search
                            â†“
                    Select Best Hyperparameters
                            â†“
Test Data (Aug 23-31)  â†’   Evaluate on Test Set
                            â†“
                    Calculate Metrics & Predictions
```

**Cross-validation**: Time Series Split (khÃ´ng shuffle)

- Fold 1: Train [Aug 1-10], Validate [Aug 11-13]
- Fold 2: Train [Aug 1-13], Validate [Aug 14-16]
- Fold 3: Train [Aug 1-16], Validate [Aug 17-19]
- ...

### TrÃ¡nh Data Leakage

âœ… **Biá»‡n phÃ¡p**:

1. **Temporal split**: KhÃ´ng Ä‘áº£o trá»™n thá»© tá»± dá»¯ liá»‡u
2. **Feature engineering trÃªn train**: TÃ­nh mean/std trÃªn train set, Ã¡p dá»¥ng trÃªn test
3. **Lag features**: Chá»‰ dÃ¹ng thÃ´ng tin tá»« quÃ¡ khá»© (khÃ´ng future data)
4. **Pipeline fit trÃªn train**: Scaler fit trÃªn train, transform trÃªn test

---

## 4. ÄÃ¡nh giÃ¡

### Metrics

| Metric    | CÃ´ng thá»©c                                                                   | Ã nghÄ©a                                  | Pháº¡m vi     |
| --------- | --------------------------------------------------------------------------- | ---------------------------------------- | ----------- |
| **MAE**   | $\frac{1}{n}\sum\|y - \hat{y}\|$                                            | Sai lá»‡ch trung bÃ¬nh (tÃ­nh báº±ng requests) | 0 - âˆ       |
| **RMSE**  | $\sqrt{\frac{1}{n}\sum(y - \hat{y})^2}$                                     | CÄƒn sai bÃ¬nh phÆ°Æ¡ng trung bÃ¬nh           | 0 - âˆ       |
| **MAPE**  | $\frac{1}{n}\sum\|\frac{y - \hat{y}}{y}\| \times 100\%$                     | Sai lá»‡ch pháº§n trÄƒm trung bÃ¬nh            | 0 - âˆ (%)   |
| **SMAPE** | $\frac{1}{n}\sum\frac{\|y - \hat{y}\|}{(\|y\|+\|\hat{y}\|)/2} \times 100\%$ | Symmetric MAPE (á»•n Ä‘á»‹nh hÆ¡n)             | 0 - 200 (%) |
| **RÂ²**    | $1 - \frac{\sum(y - \hat{y})^2}{\sum(y - \bar{y})^2}$                       | Há»‡ sá»‘ xÃ¡c Ä‘á»‹nh                           | 0 - 1       |

### Káº¿t quáº£ (vÃ­ dá»¥ - 1m timeframe)

#### Báº£ng Metrics

| Model      | MAE      | RMSE     | MAPE (%) | SMAPE (%) | RÂ²       |
| ---------- | -------- | -------- | -------- | --------- | -------- |
| XGBoost    | 45.2     | 62.3     | 8.5%     | 7.2%      | 0.92     |
| LightGBM   | 42.1     | 58.9     | 7.9%     | 6.8%      | 0.93     |
| **Hybrid** | **38.5** | **54.1** | **7.2%** | **6.3%**  | **0.94** |

**Káº¿t luáº­n**: Hybrid model cho káº¿t quáº£ tá»‘t nháº¥t trÃªn 1m timeframe.

#### Äá»“ thá»‹ Prediction vs Actual

- **Test period**: Aug 23-31
- **Visualization**:
  - ÄÆ°á»ng mÃ u xanh: Actual requests
  - ÄÆ°á»ng mÃ u Ä‘á»: Hybrid predictions
  - VÃ¹ng xÃ¡m: Khoáº£ng tin cáº­y (confidence interval)
  - CÃ¡c Ä‘iá»ƒm Ä‘á»: Anomalies phÃ¡t hiá»‡n Ä‘Æ°á»£c

### PhÃ¢n tÃ­ch lá»—i & Trade-off

#### 1. **Peak Period vs Off-Peak Accuracy**

- **Peak (1000+ requests)**: MAPE ~5% (dá»… dá»± Ä‘oÃ¡n)
- **Off-peak (<500 requests)**: MAPE ~12% (khÃ³ dá»± Ä‘oÃ¡n do noise)
- **Trade-off**: CÃ³ thá»ƒ tune láº¡i loss function vá»›i weight cao hÆ¡n cho peak periods

#### 2. **Threshold Tuning**

- **Anomaly threshold**:
  - Cao â†’ Bá» sÃ³t anomalies nhá»
  - Tháº¥p â†’ False positives nhiá»u
  - **Tá»‘i Æ°u**: Sá»­ dá»¥ng Elbow method hoáº·c ROC-AUC

#### 3. **Early Scaling vs Late Scaling Penalty**

- **Early**: Scale trÆ°á»›c khi cáº§n â†’ Chi phÃ­ cao nhÆ°ng SLA tá»‘t
- **Late**: Scale khi cáº§n â†’ Chi phÃ­ tháº¥p nhÆ°ng cÃ³ vi pháº¡m SLA
- **Cost function**: $Cost = \alpha \times SLA\_violations + \beta \times Scaling\_Cost$
  - $\alpha$: 1000 (chi phÃ­ vi pháº¡m há»£p Ä‘á»“ng)
  - $\beta$: 1 (chi phÃ­ scaling)

---

## 5. Triá»ƒn khai & Demo

### HÆ°á»›ng dáº«n cháº¡y

#### A. CÃ i Ä‘áº·t mÃ´i trÆ°á»ng

```bash
# Clone repository
git clone https://github.com/nghungvkck/DF26-NeuroFlow
cd DF26-NeuroFlow

# Táº¡o virtual environment
python -m venv .venv

# Activate (Windows)
.venv\Scripts\activate

# Activate (Linux/Mac)
source .venv/bin/activate

# CÃ i Ä‘áº·t dependencies
pip install -r demo/requirements.txt
# Hoáº·c
pip install -e .
```

#### B. Huáº¥n luyá»‡n models

```bash
# Huáº¥n luyá»‡n táº¥t cáº£ models (XGBoost + LightGBM + Hybrid)
python main.py

# Hoáº·c huáº¥n luyá»‡n riÃªng tá»«ng model
python forecasting/train/train_xgboost.py
python forecasting/train/train_lightgbm.py
python forecasting/train/train_hybrid.py
```

**Output**: Models, metrics, predictions sáº½ lÆ°u vÃ o `forecasting/artifacts/`

#### C. Cháº¡y Dashboard (Streamlit)

```bash
cd demo
streamlit run app/dashboard.py
```

- **URL**: http://localhost:8501
- **Tabs**:
  - ğŸ“Š **Overview**: Visualize raw data, bursts, events
  - ğŸ“ˆ **Forecast**: So sÃ¡nh predictions tá»« 3 models
  - âš™ï¸ **Optimization**: Scaling decisions (Predictive vs Reactive)
  - ğŸ’° **Cost Analysis**: So sÃ¡nh chi phÃ­
  - ğŸ”— **API Demo**: Test endpoints trá»±c tiáº¿p

#### D. Cháº¡y REST API Server

```bash
cd demo
python api.py
```

- **URL**: http://localhost:8000
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### API Endpoints

#### 1. **POST /forecast/metrics**

Dá»± Ä‘oÃ¡n lÆ°á»£ng requests cho timeframe tiáº¿p theo.

```bash
curl -X POST "http://localhost:8000/forecast/metrics" \
  -H "Content-Type: application/json" \
  -d '{
    "current_requests": 1200,
    "timeframe": "1m",
    "lookback_hours": 24
  }'
```

**Response**:

```json
{
  "timeframe": "1m",
  "current_requests": 1200,
  "xgboost_forecast": 1350,
  "lightgbm_forecast": 1320,
  "hybrid_forecast": 1305,
  "ensemble_forecast": 1325,
  "confidence_interval": [1200, 1450]
}
```

#### 2. **POST /recommend-scaling**

Äá» xuáº¥t sá»‘ lÆ°á»£ng servers cáº§n thiáº¿t dá»±a trÃªn forecast.

```bash
curl -X POST "http://localhost:8000/recommend-scaling" \
  -H "Content-Type: application/json" \
  -d '{
    "forecast_requests": 1400,
    "current_servers": 3,
    "capacity_per_server": 500,
    "slo_threshold": 0.85
  }'
```

**Response**:

```json
{
  "forecast_requests": 1400,
  "current_servers": 3,
  "recommended_servers": 4,
  "estimated_cpu": 70.0,
  "scaling_decision": "SCALE_UP",
  "cost_estimation": 0.2,
  "reason": "LAYER2_PREDICTIVE"
}
```

### Demo UI Screenshots

#### Dashboard - Forecast Tab

- Biá»ƒu Ä‘á»“ Ä‘Æ°á»ng (line chart) so sÃ¡nh predictions vs actual
- Legend: XGBoost (Blue), LightGBM (Green), Hybrid (Red), Actual (Black)
- Interactive: Hover xem giÃ¡ trá»‹ chi tiáº¿t, Zoom/Pan

#### Dashboard - Optimization Tab

- Scaling decisions timeline
- Cost accumulation chart
- SLA violation indicators

---

## 6. Giá»›i háº¡n & HÆ°á»›ng phÃ¡t triá»ƒn

### Giá»›i háº¡n hiá»‡n táº¡i

1. **Dá»¯ liá»‡u huáº¥n luyá»‡n cá»• (1995)**
   - KhÃ´ng pháº£n Ã¡nh hÃ nh vi ngÆ°á»i dÃ¹ng hiá»‡n Ä‘áº¡i
   - Cáº§n dá»¯ liá»‡u thá»±c táº¿ tá»« há»‡ thá»‘ng sáº£n xuáº¥t

2. **KhÃ´ng xá»­ lÃ½ Concept Drift**
   - HÃ nh vi ngÆ°á»i dÃ¹ng thay Ä‘á»•i theo thá»i gian
   - Model khÃ´ng adapt Ä‘Æ°á»£c vá»›i dá»¯ liá»‡u má»›i

3. **Giáº£ Ä‘á»‹nh cÆ¡ báº£n**
   - Giáº£ Ä‘á»‹nh linear relationship giá»¯a requests & CPU
   - KhÃ´ng tÃ­nh network I/O, disk I/O, memory

4. **KhÃ´ng cÃ³ Uncertainty Quantification**
   - KhÃ´ng cung cáº¥p khoáº£ng tin cáº­y dá»± Ä‘oÃ¡n
   - KhÃ³ quyáº¿t Ä‘á»‹nh confidence level khi scale

### Káº¿ hoáº¡ch cáº£i tiáº¿n (Roadmap)

#### **Phase 1: Drift Detection & Model Retraining**

- [ ] Implement concept drift detection (ADWIN, DDM)
- [ ] Auto-retraining pipeline (weekly/monthly)
- [ ] A/B testing Ä‘á»ƒ evaluate model updates
- **Timeline**: 2-3 thÃ¡ng

#### **Phase 2: Uncertainty Quantification**

- [ ] Quantile Regression (dá»± Ä‘oÃ¡n confidence intervals)
- [ ] Bayesian Neural Networks
- [ ] Ensemble uncertainty (variance across models)
- **Timeline**: 1-2 thÃ¡ng

#### **Phase 3: Advanced Optimization**

- [ ] Dynamic pricing integration (AWS spot price fluctuation)
- [ ] Multi-objective optimization (Pareto front)
- [ ] Reinforcement Learning (Q-Learning, Policy Gradient)
- **Timeline**: 3-4 thÃ¡ng

#### **Phase 4: System Integration**

- [ ] Kubernetes integration (auto-scale pods)
- [ ] Real-time monitoring & alerting
- [ ] Production deployment & CI/CD
- **Timeline**: 2-3 thÃ¡ng

#### **Phase 5: Cost Optimization**

- [ ] Reserved Instance capacity planning
- [ ] Spot interruption handling
- [ ] Multi-cloud optimization (AWS + Azure + GCP)
- **Timeline**: 2-3 thÃ¡ng

---

## 7. TÃ¡c Ä‘á»™ng & á»¨ng dá»¥ng

### Lá»£i Ã­ch Ä‘á»‹nh lÆ°á»£ng

| Metric                | GiÃ¡ trá»‹   | Lá»£i Ã­ch                                           |
| --------------------- | --------- | ------------------------------------------------- |
| **Giáº£m chi phÃ­**      | 25-35%    | Tiáº¿t kiá»‡m hÃ ng triá»‡u USD/nÄƒm cho doanh nghiá»‡p lá»›n |
| **Giáº£m lÃ£ng phÃ­**     | 60% â†’ 20% | Tá»« over-provisioning 60% xuá»‘ng 20%                |
| **SLA compliance**    | 99.2%+    | Vi pháº¡m < 7 giá»/nÄƒm                               |
| **Forecast accuracy** | MAPE 7-8% | Dá»± Ä‘oÃ¡n trong 7-8% sai lá»‡ch                       |
| **Response time**     | < 200ms   | Scaling decision trong 200ms                      |

### Lá»£i Ã­ch Ä‘á»‹nh tÃ­nh

1. **Tá»± Ä‘á»™ng & ThÃ´ng minh**
   - Loáº¡i bá» scaling thá»§ cÃ´ng
   - ThÃ­ch á»©ng tá»± Ä‘á»™ng vá»›i táº£i

2. **Cáº£i thiá»‡n tráº£i nghiá»‡m ngÆ°á»i dÃ¹ng**
   - Giáº£m latency (pháº£n á»©ng nhanh hÆ¡n)
   - TÄƒng uptime (Ã­t vi pháº¡m SLA)

3. **Green IT**
   - Giáº£m tiÃªu thá»¥ Ä‘iá»‡n nÄƒng
   - Giáº£m carbon footprint

### Ká»‹ch báº£n triá»ƒn khai trong doanh nghiá»‡p

#### **Ká»‹ch báº£n 1: Ecommerce Platform**

- **Hiá»‡n tráº¡ng**: Há»‡ thá»‘ng Black Friday scale-up thá»§ cÃ´ng, gÃ¢y delay cho khÃ¡ch
- **Giáº£i phÃ¡p**: Deploy hybrid autoscaler 2-3 tuáº§n trÆ°á»›c Black Friday
- **Káº¿t quáº£**: Tá»± Ä‘á»™ng scale, 99.5% SLA, tiáº¿t kiá»‡m $500K chi phÃ­ thá»«a
- **ROI**: Äáº§u tÆ° $100K (phÃ¡t triá»ƒn + triá»ƒn khai) â†’ Lá»£i nhuáº­n $400K nÄƒm 1

#### **Ká»‹ch báº£n 2: SaaS Service (Subscription Model)**

- **Hiá»‡n tráº¡ng**: Fixed capacity â†’ LÃ£ng phÃ­ 60-70% tÃ i nguyÃªn
- **Giáº£i phÃ¡p**: Chuyá»ƒn sang dynamic scaling vá»›i hybrid autoscaler
- **Káº¿t quáº£**: Giáº£m cost/user tá»« $2/thÃ¡ng â†’ $1.2/thÃ¡ng
- **ROI**: TÄƒng profit margin tá»« 20% â†’ 35%

#### **Ká»‹ch báº£n 3: Real-time Data Processing (Kafka, Spark)**

- **Hiá»‡n tráº¡ng**: Topic subscription khÃ´ng dá»± Ä‘oÃ¡n Ä‘Æ°á»£c
- **Giáº£i phÃ¡p**: Forecast topic lag, proactive scale consumer groups
- **Káº¿t quáº£**: Giáº£m lag latency tá»« 5p â†’ 30s
- **ROI**: Improve data freshness, enable real-time analytics

#### **Ká»‹ch báº£n 4: IoT & Edge Computing**

- **Hiá»‡n tráº¡ng**: Edge servers khÃ´ng predict device churn
- **Giáº£i phÃ¡p**: Forecast device connections â†’ optimize edge resources
- **Káº¿t quáº£**: Giáº£m bandwidth cost, improve response time
- **ROI**: Cost per connected device giáº£m 30%

---

## 8. TÃ¡c giáº£ & Giáº¥y phÃ©p

### Äá»™i thi

- **TÃªn Ä‘á» tÃ i**: Autoscaling Forecasting & Optimization
- **LÄ©nh vá»±c**: Machine Learning, Cloud Optimization, System Design
- **ThÃ nh viÃªn**:
- **NgÃ´n ngá»¯**: Python 3.11+
- **Thá»i gian phÃ¡t triá»ƒn**: 2026

### CÃ´ng nghá»‡ & Framework

- **ML Models**: XGBoost, LightGBM, Prophet, LSTM (TensorFlow)
- **Data Processing**: Pandas, NumPy, Scikit-learn
- **Web Framework**: FastAPI, Streamlit
- **Visualization**: Plotly, Altair
- **Config Management**: PyYAML

### License

**MIT License** - Tá»± do sá»­ dá»¥ng, sá»­a Ä‘á»•i, phÃ¢n phá»‘i vá»›i ghi nháº­n tÃ¡c giáº£

```
Copyright (c) 2025-2026

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.
```

---

---

## Project Structure Details

```
â”œâ”€â”€ data/                       # Test datasets (1m, 5m, 15m)
â”‚   â”œâ”€â”€ test_1m_autoscaling.csv
â”‚   â”œâ”€â”€ test_5m_autoscaling.csv
â”‚   â””â”€â”€ test_15m_autoscaling.csv
â”œâ”€â”€ raw_data/                   # Raw Apache HTTP logs
â”‚   â”œâ”€â”€ train.txt              # Training logs (~2.9M requests)
â”‚   â””â”€â”€ test.txt               # Test logs
â”œâ”€â”€ forecasting/                # Forecasting module
â”‚   â”œâ”€â”€ train/                  # Training scripts for 3 models
â”‚   â”‚   â”œâ”€â”€ train_lightgbm.py
â”‚   â”‚   â”œâ”€â”€ train_xgboost.py
â”‚   â”‚   â”œâ”€â”€ train_hybrid.py
â”‚   â”‚   â””â”€â”€ common.py           # Shared utilities
â”‚   â”œâ”€â”€ inference/              # Inference module for predictions
â”‚   â”‚   â””â”€â”€ predictor.py        # ModelPredictor for loading & predicting
â”‚   â”œâ”€â”€ preprocess/             # Data preprocessing pipeline
â”‚   â”‚   â”œâ”€â”€ pipeline.py         # Main orchestrator
â”‚   â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”‚   â”œâ”€â”€ parser.py
â”‚   â”‚   â”œâ”€â”€ normalizer.py
â”‚   â”‚   â”œâ”€â”€ aggregator.py
â”‚   â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”‚   â””â”€â”€ missing_handler.py
â”‚   â”œâ”€â”€ evaluate/               # Model evaluation
â”‚   â”‚   â””â”€â”€ evaluate.py         # MetricEvaluator
â”‚   â”œâ”€â”€ models/                 # Model class definitions
â”‚   â”‚   â”œâ”€â”€ base_model.py
â”‚   â”‚   â”œâ”€â”€ hybrid_model.py
â”‚   â”‚   â”œâ”€â”€ lstm_model.py
â”‚   â”‚   â”œâ”€â”€ prophet_model.py
â”‚   â”‚   â”œâ”€â”€ lightgbm_model.py
â”‚   â”‚   â”œâ”€â”€ xgboost_model.py
â”‚   â”‚   â””â”€â”€ model_factory.py
â”‚   â”œâ”€â”€ artifacts/              # Models, metrics, predictions
â”‚   â”‚   â”œâ”€â”€ models/            # Trained models (.txt, .json, .pkl, .h5)
â”‚   â”‚   â”œâ”€â”€ metrics/           # Evaluation metrics (JSON, CSV)
â”‚   â”‚   â””â”€â”€ predictions/       # Predictions CSV files
â”‚   â”œâ”€â”€ utils/                  # Utility modules
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ artifacts.py            # ArtifactManager for output organization
â”œâ”€â”€ optimization/               # Hybrid autoscaling logic
â”‚   â”œâ”€â”€ hybrid_autoscaler.py   # 4-layer autoscaler
â”‚   â”œâ”€â”€ anomaly_detection.py   # Anomaly detector
â”‚   â”œâ”€â”€ cost_model.py          # Cost estimation
â”‚   â”œâ”€â”€ metrics.py             # Scaling metrics
â”‚   â””â”€â”€ reactive_scaler.py     # Reactive scaling
â”œâ”€â”€ demo/                       # Streamlit dashboard + FastAPI
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ dashboard.py        # Main Streamlit app
â”‚   â”‚   â”œâ”€â”€ forecast_tab_simple.py
â”‚   â”‚   â”œâ”€â”€ forecast_tab_plotly.py
â”‚   â”‚   â”œâ”€â”€ optimization_tab.py
â”‚   â”‚   â””â”€â”€ api_demo_tab.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ forecast.py
â”‚   â”‚   â”œâ”€â”€ metrics_forecast.py
â”‚   â”‚   â”œâ”€â”€ scaling.py
â”‚   â”‚   â””â”€â”€ load_data.py
â”‚   â”œâ”€â”€ api.py                  # FastAPI server
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ notebook/                   # Jupyter notebooks
â”‚   â”œâ”€â”€ eda_analysis.ipynb
â”‚   â””â”€â”€ pre_process.ipynb
â”œâ”€â”€ configs/                    # Configuration files
â”‚   â””â”€â”€ train_config.yaml
â”œâ”€â”€ pyproject.toml              # Project metadata & dependencies
â”œâ”€â”€ main.py                     # Full pipeline entry point
â”œâ”€â”€ infer.py                    # Standalone inference script
â”œâ”€â”€ README.md                   # This file
â””â”€â”€ BAO_CAO_BAI_TOAN_TOI_UU.md # Technical report (Vietnamese)
```

---

## Full Predictions & Models

After running `python main.py`, all trained models and predictions are saved to `forecasting/artifacts/`:

**Models**: Binary format (XGBoost JSON, LightGBM TXT, LSTM HDF5)  
**Metrics**: Evaluation results (MAE, RMSE, MAPE, SMAPE, RÂ²) in CSV/JSON  
**Predictions**: Forecast outputs for test period in CSV

---

## Dependencies & Versions

Core dependencies from [pyproject.toml](pyproject.toml):

```
pandas>=2.0
numpy>=1.24
scikit-learn>=1.3
lightgbm>=4.0
xgboost>=2.0
prophet>=1.1
tensorflow>=2.13
fastapi>=0.109
uvicorn>=0.27
streamlit>=1.28
plotly>=5.17
```

---

## Notes & Tips

- **Data**: Historical NASA Kennedy HTTP logs (August 1995) - replace with production data for better results
- **Timeframes**: 1m is more volatile; 5m/15m are more stable
- **Cost Constants**: Edit `optimization/hybrid_autoscaler.py` for different cloud pricing
- **SLA/SLO**: CPU thresholds configurable for different SLA requirements

---

**Autoscaling Forecasting & Optimization** | MIT License | Feb 2026
